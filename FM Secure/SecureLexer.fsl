{
module SecureLexer
open FSharp.Text.Lexing
open System
open SecureParser
System.Globalization.CultureInfo.CurrentCulture <- new System.Globalization.CultureInfo("en-US")
}

// Defined macros for some needed Regular Expressions:
let digit       = ['0'-'9']
let whitespace  = [' ' '\t' '\r' '\n']
let newline     = "\n\r" | '\n' | '\r'
let char        = ['a'-'z' 'A'-'Z' '_']
let variable    = char ( char | digit )*

// Rules for recognising and building tokens in top-down
rule tokenize = parse 
// ignored tokens (skipped)
| whitespace        { tokenize lexbuf }
| newline           { lexbuf.EndPos <- lexbuf.EndPos.NextLine; tokenize lexbuf; }
// built tokens
| variable          { let str = LexBuffer<_>.LexemeString lexbuf in VARIABLE(str) }

// Input value setter, tokens for different ASTs
| '='                           { CLASIF }
| "DefinitelyNotSecure!"        { TKNC }
| "FailedAgile!ButCSMPower!"    { TKNL }

// Value delimiter between classifications
| ','                   { DELIM }

// Security Lattice Declaration to TOKENs
| "->"           { FLOW }

// End of file token
| eof           { EOF }